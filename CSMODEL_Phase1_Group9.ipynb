{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13bdb6b",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "**Name:** IoT-Based Environmental Dataset\n",
    "\n",
    "**Source:** [Kaggle - IoT-Based Environmental Dataset](https://www.kaggle.com/datasets/ziya07/iot-based-environmental-dataset?resource=download)\n",
    "\n",
    "**Summary:**  \n",
    "This dataset provides detailed environmental and mental health data collected from a university setting using IoT sensors. It includes environmental metrics such as temperature, humidity, air quality, noise, lighting, and crowd density, as well as student-reported mental health indicators like stress level, sleep hours, mood score, and mental health status. The goal is to analyze how environmental conditions may influence students' well-being.\n",
    "\n",
    "**Structure:**  \n",
    "- Each row represents a 15-minute interval of environmental readings (e.g., temperature, noise, air quality) from various campus locations.\n",
    "- The dataset contains 1000 rows and 12 columns.\n",
    "- The dataset contains the following columns:\n",
    "\n",
    "| Column Name   | Description                                      |\n",
    "|---------------|--------------------------------------------------|\n",
    "| timestamp\t| Time of environmental reading data capture (format: YYYY-MM-DD HH:MM:SS) |\n",
    "| location_id | Identifier where sensors are deployed (values range from 101-105) |\n",
    "| temperature_celsius |\tAmbient temperature in Celsius |\n",
    "| humidity_percent | Relative humidity percentage |\n",
    "| air_quality_index | Air quality measurement (higher values indicate poorer air quality) |\n",
    "| noise_level_db | Noise level in decibels |\n",
    "| lighting_lux | Illumination intensity in lux |\n",
    "| crowd_density | Number of people in the area |\n",
    "| stress_level | Modeled student stress score (0–100) |\n",
    "| sleep_hours | Estimated sleep duration in hours |\n",
    "| mood_score | Modeled emotional score ranging from -3 (very negative) to +3 (very positive) |\n",
    "| mental_health_status | Categorical indicator (0 = Normal, 1 = Mild Risk, 2 = At Risk) |\n",
    "\n",
    "**Provenance:**  \n",
    "Compiled and published by Ziya on Kaggle. Last updated in 2025.\n",
    "\n",
    "**License:**  \n",
    "Check the Kaggle page for licensing details; the dataset is typically available for educational and non-commercial use.\n",
    "\n",
    "**Note:**  \n",
    "- The location_id column refers to the specific IoT sensor or monitored area within the university environment.\n",
    "- The dataset was likely compiled from various environmental sensors and self-reported student responses, then structured into a CSV file.\n",
    "- mental_health_status is a simplified binary label and may not capture the full complexity of a student's psychological condition.\n",
    "\n",
    "**Potential Implications and Biases:**\n",
    "- Since the data involves self-reported mental health metrics, responses may be subject to personal bias, underreporting, or overestimation.\n",
    "- Sensor accuracy and calibration may affect the consistency and precision of environmental measurements (e.g., noise or air quality).\n",
    "- The dataset is limited to a university population and may not generalize to broader demographic or institutional contexts.\n",
    "- Environmental conditions are highly dynamic, and snapshots in time may not fully capture long-term exposure or effects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d75150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 12)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "mental_health_df = pd.read_csv('university_mental_health_iot_dataset.csv')\n",
    "mental_health_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b74646",
   "metadata": {},
   "source": [
    "**The dataset contains 1000 observations through the pandas 'shape' attribute.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8036f",
   "metadata": {},
   "source": [
    "## Target Research Questions\n",
    "\n",
    "**1. To what extent do environmental factors (temperature, humidity, air quality, noise, lighting, crowd density) predict student stress levels, and which factors have the strongest relationship?**\n",
    "\n",
    "EDA: \n",
    "1. What is the correlation between each environmental factor and student stress levels?\n",
    "2. Which environmental factor has the strongest relationship with stress levels?\n",
    "3. Are there any non-linear relationships between environmental factors and stress?\n",
    "4. Do certain locations consistently show higher stress levels regardless of environmental conditions?\n",
    "\n",
    "**2. How does sleep duration interact with environmental conditions to influence student mood scores, and can specific thresholds of sleep be identified that buffer against negative environmental effects?**\n",
    "\n",
    "EDA: \n",
    "1. How does mood score vary with sleep hours among students exposed to high vs. low environmental stressors?\n",
    "2. Is there a threshold of sleep duration that appears to protect against negative environmental effects?\n",
    "3. Which environmental factor shows the strongest interaction with sleep duration in predicting mood?\n",
    "4. Does the relationship between sleep and mood differ across different campus locations?\n",
    "\n",
    "**3. What combination of environmental conditions differentiates locations with higher rates of mental health risk (status 1-2) from those with predominantly normal status (0), and can these patterns inform university space design?**\n",
    "\n",
    "EDA: \n",
    "1. What are the average values of environmental variables in locations/times where mental health status is at risk (1) vs. normal (0)?\n",
    "2. Which locations have the highest proportion of at-risk mental health statuses?\n",
    "3. Are there specific combinations of environmental factors that are consistently associated with at-risk mental health status?\n",
    "4. Do temporal patterns exist in the relationship between environmental conditions and mental health status?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5facc2",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Our data preprocessing approach ensures that the dataset is accurate, consistent, and fully prepared for analysis. We started by inspecting the data for missing values and duplicates, confirming that the dataset is complete and unique. Next, we standardized numerical precision across relevant columns, rounding values to appropriate decimal places to enhance consistency and interpretability. We also converted key columns—such as timestamps and categorical variables—to their correct data types, enabling more effective time-based and group analyses. Outlier detection was performed using summary statistics, allowing us to identify any unusual or potentially problematic values. Each transformation was carefully documented and verified, resulting in a clean and reliable dataset that provides a solid foundation for meaningful exploratory analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23611ef",
   "metadata": {},
   "source": [
    "It is noticed that the decimal inconsistency in the specified columns: **temperature_celsius**, **humidity_percent**, **noise_level_db**, and **sleep_hours** constitutes a data integrity issue affecting numerical precision and standardization. Through data cleaning, specifically rounding to uniform decimal digits the dataset’s quality, reliability, and analytical value are significantly enhanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08f48b",
   "metadata": {},
   "source": [
    "The function **'min_decimal_places'** is used to identify the lowest number of decimal digits present within each specified column. This lowest decimal digit is then applied uniformly to all values in the column, ensuring consistent decimal formatting. Such standardization improves data integrity and facilitates accurate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f723ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_decimal_places(series):\n",
    "    series = series.dropna()\n",
    "    decimals = series.astype(str).apply(\n",
    "        lambda x: len(x.split('.')[-1]) if '.' in x else 0\n",
    "    )\n",
    "    return decimals.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c501052",
   "metadata": {},
   "source": [
    "The function **'implement_min_decimal'** is used to implement the data cleaning basing on the returned number of decimal digits from 'min_decimal_places' function and then rounded off if the min_decimal_places value is < than the data's decimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6d995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_min_decimal(column):\n",
    "    cleaned_column = column.round(min_decimal_places(column))\n",
    "    return cleaned_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76a331",
   "metadata": {},
   "source": [
    "**Implementation of the funtions to the columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afd48d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   temperature_celsius  humidity_percent  noise_level_db  sleep_hours\n",
      "0            24.328184         62.987529       54.429034          7.2\n",
      "1            26.177300         52.482089       62.336451          5.0\n",
      "2            25.443028         55.736424       51.967691          5.0\n",
      "3            24.671652         71.484457       54.658851          8.2\n",
      "4            21.628577         61.132704       60.623440          6.6\n",
      "5            25.414405         45.617220       51.121999          6.5\n",
      "6            29.646073         69.192289       52.022026          7.6\n",
      "7            28.036260         53.318559       43.232965          9.7\n",
      "8            28.779560         78.732978       54.850632          5.8\n",
      "9            22.466353         70.800481       51.651515          7.6\n"
     ]
    }
   ],
   "source": [
    "# List of columns to clean (excluding mood_score and mental_health_status if not needed)\n",
    "columns_to_clean = [\n",
    "    'temperature_celsius', 'humidity_percent', 'noise_level_db', 'sleep_hours'\n",
    "]\n",
    "\n",
    "# Apply implement_min_decimal to each column in the list\n",
    "for col in columns_to_clean:\n",
    "    mental_health_df[col] = implement_min_decimal(mental_health_df[col])\n",
    "\n",
    "# Verification for cleaned columns\n",
    "print(mental_health_df[columns_to_clean].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a90029",
   "metadata": {},
   "source": [
    "**Lighting Lux Standardization**\n",
    "\n",
    "To ensure consistency and interpretability, we round the **lighting_lux** values to the nearest integer. Lux, as a unit of light intensity, is almost always measured and reported as a whole number in both sensor outputs and practical applications. This step removes insignificant decimal places, making the data cleaner and easier to analyze without sacrificing any meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65aa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_health_df['lighting_lux'] = mental_health_df['lighting_lux'].round(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2162db",
   "metadata": {},
   "source": [
    "**Categorical Variable Conversion**\n",
    "\n",
    "We convert the **location_id** and **mental_health_status** columns to categorical data types. This not only optimizes memory usage but also clarifies the nature of these variables as discrete categories rather than continuous numbers. Properly encoding categorical variables is essential for accurate analysis and modeling, especially when performing group-based operations or preparing data for machine learning algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f227898",
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_health_df['location_id'] = mental_health_df['location_id'].astype('category')\n",
    "mental_health_df['mental_health_status'] = mental_health_df['mental_health_status'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbede56",
   "metadata": {},
   "source": [
    "**Timestamp Parsing**\n",
    "\n",
    "We convert the **timestamp** column from string format to a datetime object. This transformation enables us to perform time-based analyses, such as identifying trends over specific periods, aggregating data by hour or day, and visualizing temporal patterns. Accurate datetime formatting is foundational for any analysis involving time series or temporal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63832cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_health_df['timestamp'] = pd.to_datetime(mental_health_df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1ab85",
   "metadata": {},
   "source": [
    "**Missing Values and Duplicate Checks**\n",
    "\n",
    "Before further processing, we check for missing values and duplicate rows in the dataset. Addressing these issues at the outset prevents potential biases and errors in our analysis. If any are found, we can decide whether to impute, remove, or otherwise handle them based on the context and extent of the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2187082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp               0\n",
      "location_id             0\n",
      "temperature_celsius     0\n",
      "humidity_percent        0\n",
      "air_quality_index       0\n",
      "noise_level_db          0\n",
      "lighting_lux            0\n",
      "crowd_density           0\n",
      "stress_level            0\n",
      "sleep_hours             0\n",
      "mood_score              0\n",
      "mental_health_status    0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(mental_health_df.isna().sum())\n",
    "print(mental_health_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b70a45",
   "metadata": {},
   "source": [
    "**Result:** \n",
    " \n",
    "The output above shows that there are **no missing values** and **no duplicate rows** in the dataset. This confirms that the data is complete and unique, allowing us to proceed confidently with further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6005e7",
   "metadata": {},
   "source": [
    "**Outlier Detection**\n",
    "\n",
    "We examine summary statistics and visualizations for each numerical column to identify potential outliers or anomalous values. Detecting and addressing outliers is crucial, as they can disproportionately influence statistical analyses and model performance. Depending on the findings, we may choose to investigate, correct, or exclude these values from further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bbf9d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 timestamp  temperature_celsius  humidity_percent  \\\n",
      "count                 1000          1000.000000       1000.000000   \n",
      "mean   2024-05-06 12:52:30            24.208457         60.189320   \n",
      "min    2024-05-01 08:00:00            15.235949         29.804878   \n",
      "25%    2024-05-03 22:26:15            22.183079         53.726462   \n",
      "50%    2024-05-06 12:52:30            24.168562         60.048989   \n",
      "75%    2024-05-09 03:18:45            26.125555         66.726456   \n",
      "max    2024-05-11 17:45:00            33.579323         91.377485   \n",
      "std                    NaN             3.012801          9.747296   \n",
      "\n",
      "       air_quality_index  noise_level_db  lighting_lux  crowd_density  \\\n",
      "count        1000.000000     1000.000000    1000.00000    1000.000000   \n",
      "mean           85.446000       54.722387     301.49300      31.736000   \n",
      "min            20.000000       24.540712     155.00000       5.000000   \n",
      "25%            51.000000       47.853538     267.00000      18.000000   \n",
      "50%            86.000000       54.777010     300.50000      31.000000   \n",
      "75%           119.000000       61.794293     334.00000      46.000000   \n",
      "max           149.000000       85.926413     503.00000      59.000000   \n",
      "std            37.970526       10.051077      48.87157      15.757366   \n",
      "\n",
      "       stress_level  sleep_hours   mood_score  \n",
      "count   1000.000000  1000.000000  1000.000000  \n",
      "mean      39.086000     6.423800     1.641000  \n",
      "min        1.000000     3.000000    -2.200000  \n",
      "25%       29.000000     5.600000     1.000000  \n",
      "50%       39.000000     6.400000     1.700000  \n",
      "75%       49.000000     7.200000     2.500000  \n",
      "max       78.000000    10.000000     3.000000  \n",
      "std       13.416057     1.169915     1.016076  \n"
     ]
    }
   ],
   "source": [
    "print(mental_health_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2277a0",
   "metadata": {},
   "source": [
    "**Outlier Handling**\n",
    "\n",
    "After identifying potential outliers using summary statistics, we proceed to handle them to ensure they do not unduly influence our analysis. We use the Interquartile Range (IQR) method to detect outliers for each numerical column. Values lying below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are considered outliers. We will flag these outliers and, for this analysis, remove them to maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b603ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_celsius: Removed 8 outliers\n",
      "humidity_percent: Removed 11 outliers\n",
      "air_quality_index: Removed 0 outliers\n",
      "noise_level_db: Removed 7 outliers\n",
      "lighting_lux: Removed 7 outliers\n",
      "crowd_density: Removed 0 outliers\n",
      "stress_level: Removed 1 outliers\n",
      "sleep_hours: Removed 7 outliers\n",
      "mood_score: Removed 3 outliers\n",
      "New dataset shape after outlier removal: (956, 12)\n"
     ]
    }
   ],
   "source": [
    "# List of numerical columns to check for outliers\n",
    "num_cols = [\n",
    "    'temperature_celsius', 'humidity_percent', 'air_quality_index',\n",
    "    'noise_level_db', 'lighting_lux', 'crowd_density',\n",
    "    'stress_level', 'sleep_hours', 'mood_score'\n",
    "]\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        before = df.shape[0]\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "        after = df.shape[0]\n",
    "        print(f\"{col}: Removed {before - after} outliers\")\n",
    "    return df\n",
    "\n",
    "# Remove outliers\n",
    "mental_health_df_clean = remove_outliers_iqr(mental_health_df, num_cols)\n",
    "\n",
    "# Show the new shape of the cleaned dataset\n",
    "print(\"New dataset shape after outlier removal:\", mental_health_df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955c471",
   "metadata": {},
   "source": [
    "**Result**  \n",
    "Outliers have been removed from the dataset using the IQR method for each numerical column. The updated dataset is now less likely to be influenced by extreme values, ensuring more robust and reliable analysis in subsequent steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
